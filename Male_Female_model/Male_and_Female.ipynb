{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750091b9-76a1-456f-93c3-dae72746d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "176299e1-fd39-4900-8eb5-4d3a0785b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory path containing the dataset of images.\n",
    "# The dataset is organized into two categories: 'Male' and 'Female'.\n",
    "\n",
    "path1 = r\"C:\\Male and Female\"\n",
    "cate = ['Female', 'Male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "91fa81da-0b5e-4ce3-8a5d-4597d196f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 200 # Set the target image size for resizing all images in the dataset.\n",
    "input_image = [] # Initialize an empty list to store the image data along with their corresponding labels.\n",
    "\n",
    "# Loop through each category in the dataset (e.g., 'Mask' and 'No_Mask').\n",
    "for i in cate:\n",
    "    folders = os.path.join(path1, i) # Construct the full path to the folder corresponding to the current category.\n",
    "    label = cate.index(i)   # Assign a numeric label to the category (e.g., 0 for 'Mask', 1 for 'No_Mask').\n",
    "\n",
    "    # Iterate through all the images in the category's folder.\n",
    "    for image in os.listdir(folders):\n",
    "        image_path = os.path.join(folders, image)\n",
    "        image_array = cv2.imread(image_path)\n",
    "        image_array = cv2.resize(image_array, (image_size, image_size))\n",
    "        input_image.append([image_array, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d76324e5-e814-48f9-aacd-b7c063eee0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 95, 121, 133],\n",
       "         [ 43,  72,  88],\n",
       "         [ 40,  47,  76],\n",
       "         ...,\n",
       "         [ 26,  34,  64],\n",
       "         [ 24,  30,  61],\n",
       "         [ 39,  47,  78]],\n",
       " \n",
       "        [[ 80, 118, 130],\n",
       "         [ 40,  65,  87],\n",
       "         [ 39,  44,  75],\n",
       "         ...,\n",
       "         [ 24,  33,  70],\n",
       "         [ 33,  46,  82],\n",
       "         [ 45,  58,  90]],\n",
       " \n",
       "        [[ 64, 101, 113],\n",
       "         [ 35,  51,  75],\n",
       "         [ 35,  47,  75],\n",
       "         ...,\n",
       "         [ 27,  35,  73],\n",
       "         [ 25,  39,  71],\n",
       "         [ 38,  60,  88]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[137, 154, 207],\n",
       "         [133, 152, 204],\n",
       "         [130, 151, 205],\n",
       "         ...,\n",
       "         [155, 185, 236],\n",
       "         [149, 182, 232],\n",
       "         [151, 184, 234]],\n",
       " \n",
       "        [[136, 158, 210],\n",
       "         [129, 150, 203],\n",
       "         [132, 153, 205],\n",
       "         ...,\n",
       "         [154, 185, 234],\n",
       "         [152, 185, 233],\n",
       "         [153, 186, 235]],\n",
       " \n",
       "        [[134, 156, 208],\n",
       "         [136, 157, 208],\n",
       "         [134, 152, 205],\n",
       "         ...,\n",
       "         [155, 185, 234],\n",
       "         [159, 189, 238],\n",
       "         [156, 186, 235]]], dtype=uint8),\n",
       " 0]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ed4ed9af-5641-47bf-9093-618989b44c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(input_image) # Shuffle the input_image list randomly to ensure the data is not ordered by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "51212028-27fe-4e8d-989b-a9734f814c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []  # Initialize empty lists to store features (X) and labels (Y)\n",
    "Y = []  # Loop through each pair (X_values, labels) in the input_image dataset\n",
    "\n",
    "for X_values, labels in input_image:\n",
    "    X.append(X_values)\n",
    "    Y.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1b9682c6-b307-470d-b321-b93fb51c29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of features (X) into a NumPy array for efficient numerical operations\n",
    "X = np.array(X)\n",
    "\n",
    "# Convert the list of labels (Y) into a NumPy array for easier handling in machine learning algorithms\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "198d8469-e542-414c-8dd2-f4455be55afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the input_image dataset into training and testing subsets\n",
    "\n",
    "train = input_image[0:4334]\n",
    "test = input_image[4335:5418]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f64faac4-c43c-4ed0-a1cf-c052e3ff507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1083"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4ecbc561-f2a8-46bd-858e-5941122316a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4334, 1083)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "da66f633-844e-4268-bc85-833ef07195d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store training features (X_train) and training labels (Y_train)\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# Loop through each pair (X_values, labels) in the train dataset\n",
    "for X_values, labels in train:\n",
    "    \n",
    "    X_train.append(X_values) # Append the feature values (X_values) to the X_train list\n",
    "    \n",
    "    Y_train.append(labels)  # Append the corresponding label to the Y_train list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "21724e83-4a01-40fc-903c-fbd362d3b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store testing features (X_test) and testing labels (Y_test)\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "# Loop through each pair (X_values, labels) in the test dataset\n",
    "for X_values, labels in test:\n",
    "    X_test.append(X_values)\n",
    "    Y_test.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c65c3c83-8681-485c-a1eb-54b40c655792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of training features (X_train) into a NumPy array for efficient numerical operations\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# Convert the list of training labels (Y_train) into a NumPy array for easier handling in machine learning models\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "# Convert the list of testing features (X_test) into a NumPy array for efficient numerical operations\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0648d400-0563-4d49-b40e-cedcf2f9b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values of the training set by dividing each pixel by 255 (scaling to the range [0, 1])\n",
    "X_train = X_train / 255\n",
    "\n",
    "# Normalize the pixel values of the testing set by dividing each pixel by 255 (scaling to the range [0, 1])\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1376296b-bada-44e3-ac9e-0d2864c781b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# This code defines a Convolutional Neural Network (CNN) with four convolutional layers,\n",
    "# each followed by max-pooling, and two fully connected layers for classifying images into two categories.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size =(5,5), activation = 'relu', padding = 'same',))\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size =(5,5), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size =(5,5), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size =(5,5), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation = 'relu', input_shape = X_train.shape[1:]))\n",
    "model.add(Dense(2, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2a5c9351-0d12-490d-895d-902c0bee2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy']) # Compiling the model with the Adam optimizer, sparse categorical cross-entropy loss, and accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4b0191b4-2184-4da5-b92d-414c700ba653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - accuracy: 0.5981 - loss: 0.6566 - val_accuracy: 0.7512 - val_loss: 0.5090\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 919ms/step - accuracy: 0.7786 - loss: 0.4661 - val_accuracy: 0.8318 - val_loss: 0.3919\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 914ms/step - accuracy: 0.8669 - loss: 0.3156 - val_accuracy: 0.8756 - val_loss: 0.2810\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 940ms/step - accuracy: 0.9073 - loss: 0.2314 - val_accuracy: 0.9032 - val_loss: 0.2208\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 966ms/step - accuracy: 0.9460 - loss: 0.1357 - val_accuracy: 0.9539 - val_loss: 0.1569\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 952ms/step - accuracy: 0.9720 - loss: 0.0812 - val_accuracy: 0.9585 - val_loss: 0.1196\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 985ms/step - accuracy: 0.9906 - loss: 0.0319 - val_accuracy: 0.9700 - val_loss: 0.0964\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 961ms/step - accuracy: 0.9881 - loss: 0.0352 - val_accuracy: 0.9747 - val_loss: 0.1113\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 946ms/step - accuracy: 0.9908 - loss: 0.0268 - val_accuracy: 0.9793 - val_loss: 0.1022\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 956ms/step - accuracy: 0.9997 - loss: 0.0041 - val_accuracy: 0.9839 - val_loss: 0.0948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2cc91e06300>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=64, epochs = 10, validation_split= .1) # Training the model on the training data with the specified batch size, number of epochs, and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0757a266-c876-40f0-90ba-f1a551e1acf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to predict the output (class probabilities) for the test set\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6b329818-2d27-46a8-afdb-a30c1f8db984",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_cate = pred_test.argmax(axis = 1)  # Convert predicted probabilities to class labels by selecting the index of the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "03373d25-76e2-4be8-bd7f-3b39e1f19eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dd80b170-42cc-446e-a7c0-025e62992079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       559\n",
      "           1       0.98      0.98      0.98       524\n",
      "\n",
      "    accuracy                           0.98      1083\n",
      "   macro avg       0.98      0.98      0.98      1083\n",
      "weighted avg       0.98      0.98      0.98      1083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tab = confusion_matrix(Y_test, pred_test_cate)  # Compute the confusion matrix to compare predicted vs. true labels\n",
    "print(classification_report(Y_test, pred_test_cate))  # Print the classification report with precision, recall, F1-score, and support for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e9fb7d21-7b21-4684-b29a-c119edcabd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787626962142197"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab.diagonal().sum()/tab.sum() # Calculate the accuracy by dividing the sum of correct predictions (diagonal elements) by the total number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277375ca-9d7c-464b-873d-cc2efe6f7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Male_Female_model.h5') # Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5398ea89-af98-46de-98f3-cc98fd105d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# This code captures video from a webcam, detects faces using a Haar Cascade classifier,\n",
    "# processes the faces through a pre-trained CNN model to predict if the person is male or female, \n",
    "# and displays the results with labeled rectangles around the faces.\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model=load_model(\"Male_Female_model.h5\")\n",
    "\n",
    "results={0:'Female',1:'Male'}\n",
    "GR_dict={0:(0,255,255),1:(0,255,0)}\n",
    "\n",
    "rect_size = 4\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "\n",
    "haarcascade = cv2.CascadeClassifier(r\"C:\\Users\\ASUS\\OneDrive\\Desktop\\Imarticus AI and ML\\Haarcascade\\haarcascade_frontalface_default.xml\")\n",
    "while True:\n",
    "    (rval, im) = cap.read()\n",
    "    im=cv2.flip(im,1,1) \n",
    "\n",
    "    \n",
    "    rerect_size = cv2.resize(im, (im.shape[1] // rect_size, im.shape[0] // rect_size))\n",
    "    faces = haarcascade.detectMultiScale(rerect_size)\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * rect_size for v in f]\n",
    "\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        rerect_sized=cv2.resize(face_img,(200,200)) #  same val as used in cnn\n",
    "        normalized=rerect_sized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,200,200,3)) #  same val as used in cnn\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),GR_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),GR_dict[label],-1)\n",
    "        cv2.putText(im, results[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,0,0),2)\n",
    "\n",
    "    cv2.imshow('Liv Camera',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    \n",
    "    if key == 27: # use the escape key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef4bcc-bc8b-4778-bdbb-f218b8e73020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
